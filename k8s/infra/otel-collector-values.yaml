# OpenTelemetry Collector Helm values for GRUD project
# Receives OTLP metrics from Go services and exports to Prometheus

mode: deployment

# Image configuration
image:
  repository: otel/opentelemetry-collector-contrib

# Resources
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

# Replicas
replicaCount: 1

# Schedule on infra node
nodeSelector:
  node-type: infra

tolerations:
  - key: "workload"
    operator: "Equal"
    value: "infra"
    effect: "NoSchedule"

# Service configuration
service:
  type: ClusterIP

# Ports
ports:
  # OTLP gRPC receiver (services send metrics here)
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
  # OTLP HTTP receiver
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  # Prometheus metrics endpoint (for scraping)
  prometheus:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP
  # Prometheus exporter (exports scraped metrics)
  prom-exporter:
    enabled: true
    containerPort: 8889
    servicePort: 8889
    protocol: TCP

# OpenTelemetry Collector configuration
config:
  receivers:
    # Receive OTLP metrics from Go services
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

  processors:
    # Batch processor for better performance
    batch:
      timeout: 10s
      send_batch_size: 1024

    # Add resource attributes
    resource:
      attributes:
        - key: cluster.name
          value: grud-cluster
          action: upsert

    # Memory limiter to prevent OOM
    memory_limiter:
      check_interval: 1s
      limit_mib: 400

  exporters:
    # Export metrics to Prometheus
    prometheus:
      endpoint: "0.0.0.0:8889"
      namespace: grud
      const_labels:
        cluster: grud-cluster
      # Convert resource attributes (service.name, service.version) to metric labels
      resource_to_telemetry_conversion:
        enabled: true

    # Export traces to Tempo
    otlp/tempo:
      endpoint: tempo.infra.svc.cluster.local:4317
      tls:
        insecure: true

    # Debug logging (optional, disable in production)
    debug:
      verbosity: basic

  service:
    pipelines:
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch, resource]
        exporters: [prometheus]
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch, resource]
        exporters: [otlp/tempo]

# ServiceMonitor for Prometheus to scrape OTel Collector
serviceMonitor:
  enabled: true
  extraLabels:
    release: prometheus
  metricsEndpoints:
    - port: prom-exporter
      interval: 5s
